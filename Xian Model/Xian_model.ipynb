{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from keras.layers import Concatenate, Dense, Input\n",
    "from tensorflow.keras.layers import Conv2D, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D, MaxPooling1D, AveragePooling1D\n",
    "from tensorflow.keras.layers import Activation, LayerNormalization\n",
    "from tensorflow.keras.layers import Normalization\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Layer, Lambda\n",
    "# import keras.backend as K\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import time\n",
    "# import sys\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import gc\n",
    "# import glob\n",
    "# import os, datetime\n",
    "# import matplotlib.pyplot as plt\n",
    "# from tensorflow import keras\n",
    "import pickle as pkl\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "# from sklearn.metrics import mean_absolute_percentage_error\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# import numpy.linalg as la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train = pd.read_csv('G:\\Linux_files_packup\\Paper3_Datasets\\Xian Dataset\\df_Train2scaled3.csv', nrows= 500000)\n",
    "\n",
    "df_Test  = pd.read_csv('G:\\Linux_files_packup\\Paper3_Datasets\\Xian Dataset\\df_Test2scaled3.csv', nrows= 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Train.info()\n",
    "# df_Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Train['Start_hour2'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Test['Start_hour2'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Train.drop(['Start_day2', 'Start_hour2'], axis=1, inplace=True)\n",
    "# df_Test.drop(['Start_day2', 'Start_hour2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividing the train and test datasets into spatial, temporal, and external feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(df,h):\n",
    "    df_new = df[df['Start_hour2']==h]\n",
    "    \n",
    "    ST_features= ['pick_Latitude', 'pick_Longitude', 'drop_Latitude','drop_Longitude', 'N_hops', 'Day', 'P_Hour',\n",
    "    'haversine_distance','speed', 'week_number_of_year', 'dayofyear', 'pickup_time','pickup_cluster', \n",
    "    'dropoff_cluster', 'Heat_score_cluster','Heat_score_grid', 'PUlocation_Grid3', 'DOlocation_Grid3',\n",
    "    'dayofweek_0', 'dayofweek_1', 'dayofweek_2','dayofweek_3', 'dayofweek_4', 'dayofweek_5', 'dayofweek_6',\n",
    "    'Start_hour','Start_day', 'pick_local','pick_neighbors1', 'pick_neighbors2', 'pick_neighbors3',\n",
    "    'pick_neighbors4','pick_neighbors5','pick_neighbors6','pick_neighbors7', 'pick_neighbors8']\n",
    "    \n",
    "    \n",
    "    External = ['car_id', 'weekends_0', 'weekends_1', 'Holiday_0', 'Holiday_1','Temp', 'Weather', 'Wind',\n",
    "            'Humidity', 'Visibility']\n",
    "    Target = ['Trip_duration']\n",
    "\n",
    "    ##XTrain\n",
    "    df_XTrain_ST       = df_new.iloc[:,:][ST_features]\n",
    "    df_XTrain_External      = df_new.iloc[:,:][External]\n",
    "    y_train1  =  df_new.iloc[:,:][Target]\n",
    "    \n",
    "    df_XTrain_ST       = df_XTrain_ST.to_numpy()\n",
    "    df_XTrain_External      = df_XTrain_External.to_numpy()\n",
    "#     y_train1 =  y_train1.to_numpy()\n",
    "    \n",
    "    ####Train:\n",
    "    ####### ST\n",
    "    sample_size = df_XTrain_ST.shape[0] # number of samples in train set\n",
    "    time_steps  = df_XTrain_ST.shape[1] # number of features in train set\n",
    "    input_dimension = 1               # each feature is represented by 1 number\n",
    "\n",
    "    train_ST_emb_reshaped = df_XTrain_ST.reshape(sample_size,time_steps,input_dimension)\n",
    "    \n",
    "\n",
    "    return train_ST_emb_reshaped, df_XTrain_External, y_train1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention():\n",
    "    \n",
    "    def __init__(self, attn_dropout=0.0):\n",
    "        self.dropout = Dropout(attn_dropout)\n",
    "        self.activation = Activation('selu')\n",
    "        \n",
    "        \n",
    "    def __call__(self, q, k, v, mask):\n",
    "        \n",
    "        qk = tf.matmul(q, k, transpose_b=True)\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention = qk / tf.math.sqrt(dk)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "        weights = self.activation(scaled_attention)\n",
    "        output = tf.matmul(weights, v)\n",
    "\n",
    "        return output, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Graph Model with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import graph adjacency matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Adj_distance\n",
    "with open(r'G:\\Linux_files_packup\\Paper3_Datasets\\Xian_adj_matrices\\New_Semantic_Graph\\Adj_distance_scaled.npy', 'rb') as f:\n",
    "    Adj_distance = np.load(f)\n",
    "    \n",
    "    \n",
    "####Adj_Speed\n",
    "with open(r'G:\\Linux_files_packup\\Paper3_Datasets\\Xian_adj_matrices\\New_Semantic_Graph\\Adj_speed_scaled.npy', 'rb') as f:\n",
    "    Adj_Speed = np.load(f)\n",
    "    \n",
    "\n",
    "####Adj_Flow\n",
    "with open(r'G:\\Linux_files_packup\\Paper3_Datasets\\Xian_adj_matrices\\New_Semantic_Graph\\Adj_Flow_scaled.npy', 'rb') as f:\n",
    "    Adj_Flow = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adj_distance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(Adj_distance[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "def normalized_adj(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    normalized_adj = adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "    normalized_adj = normalized_adj.astype(np.float64)\n",
    "    return normalized_adj\n",
    "    \n",
    "def sparse_to_tuple(mx):\n",
    "    mx = mx.tocoo()\n",
    "    coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "    L = tf.SparseTensor(coords, mx.data, mx.shape)\n",
    "    return tf.sparse_reorder(L) \n",
    "    \n",
    "def calculate_laplacian(adj, lambda_max=1):  \n",
    "    adj = normalized_adj(adj + sp.eye(adj.shape[0]))\n",
    "    adj = sp.csr_matrix(adj)\n",
    "    adj = adj.astype(np.float64)\n",
    "    return sparse_to_tuple(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided two tensors t1 and t2\n",
    "# Euclidean distance = sqrt(sum(square(t1-t2)))\n",
    "def euclidean_distance(vects):\n",
    "    \"\"\"Find the Euclidean distance between two vectors.\n",
    "\n",
    "    Arguments:\n",
    "        vects: List containing two tensors of same length.\n",
    "\n",
    "    Returns:\n",
    "        Tensor containing euclidean distance\n",
    "        (as floating point value) between vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    x, y = vects\n",
    "    sum_square = tf.math.reduce_sum(tf.math.square(x - y), axis=1, keepdims=True)\n",
    "    return tf.math.sqrt(tf.math.maximum(sum_square, tf.keras.backend.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph block For Global correlations (Adj_graph matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable_glorot(input_dim, output_dim, name=\"\"):\n",
    "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "#     print(init_range)\n",
    "    initial = tf.random_uniform([batch_size,input_dim, output_dim], minval=-init_range,\n",
    "                            maxval=init_range, dtype=tf.float32)\n",
    "\n",
    "    return tf.Variable(initial,name=name)#, init_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G_GCN(object):     \n",
    "    def __init__(self, num_units, adj, inputs, output_dim,dim,weightsx,weightsx2\n",
    "                 ,activationx = tf.nn.relu, input_size = None, num_proj=None, reuse = True, **kwargs):\n",
    "        super(G_GCN, self).__init__(**kwargs)\n",
    "        if input_size is not None:\n",
    "            logging.warn(\"%s: The input_size parameter is deprecated.\", self)\n",
    "        self._num_units = num_units\n",
    "        self._output_dim = output_dim\n",
    "        self._inputs = inputs\n",
    "        self._num_nodes = adj.shape[0]\n",
    "        self._input_dim = dim\n",
    "        self._batch_size = tf.shape(inputs)[0]\n",
    "        self._adj = adj  \n",
    "        self.weightsx = weightsx\n",
    "        self._activation = activationx\n",
    "        self.weightsx1 = weightsx2\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def _build_sparse_matrix(L):\n",
    "        L = L.tocoo()\n",
    "        indices = np.column_stack((L.row, L.col))\n",
    "        L = tf.SparseTensor(indices, L.data, L.shape)\n",
    "        return tf.sparse_reorder(L)\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        output_size = self._num_units\n",
    "        return output_size\n",
    "        \n",
    "    def init_state(self,batch_size):       \n",
    "        state = tf.zeros(shape=[batch_size, self._num_nodes*self._num_units], dtype=tf.float32)\n",
    "        return state  \n",
    "               \n",
    "    @staticmethod\n",
    "    def _concat(x, x_):\n",
    "        x_ = tf.expand_dims(x_, 0)\n",
    "        return tf.concat([x, x_], axis=0)   \n",
    "           \n",
    "    def _gconv(self,scope=None):\n",
    "        ####[batch, num_nodes, seq]\n",
    "        inputs = self._inputs       \n",
    "#         inputs = tf.transpose(inputs) #, perm=[2,0,1])\n",
    "#        print('0',inputs.get_shape())\n",
    "        x0 = inputs #tf.reshape(inputs,shape=[self._num_nodes,self._batch_size*self._input_dim])\n",
    "#         print(\"X0:\",x0)\n",
    "        scope = tf.get_variable_scope()\n",
    "        with tf.variable_scope(scope, reuse=True):\n",
    "            ####hidden1\n",
    "            for adj in self._adj:\n",
    "                adjx = tf.convert_to_tensor(adj,dtype=tf.dtypes.float32)\n",
    "                x1 = adjx*x0[...,None]\n",
    "#             print(x1)\n",
    "#             x1 = tf.reshape(x1,shape=[self._num_nodes,self._batch_size,self._input_dim])\n",
    "#             x1 = tf.reshape(x1,shape=[self._num_nodes*self._batch_size,self._input_dim])\n",
    "            \n",
    "#             weights = weight_variable_glorot(self._input_dim, self.output_size, name='weights')\n",
    "#             print(\"x1:\",x1)\n",
    "#             weightse = tf.get_variable(\n",
    "#                 'weightse', [self._input_dim, 135,135], initializer = tf.truncated_normal_initializer(stddev=0.1), dtype= tf.float64)\n",
    "#             print(\"weightse 1:\",weightse)\n",
    "#             self.hidden = self._activation(tf.matmul(x1,weightse))\n",
    "            self.output = self._activation(x1 * self.weightsx)\n",
    "#             self.output = self._activation(tf.matmul(x1,self.weightsx) + self.bias)\n",
    "#             print(self.output)\n",
    "#             self.output2 = self._activation(self.output * self.weightsx1)\n",
    "#             self.output2 = self._activation(tf.matmul(self.output,self.weightsx1) + self.bias1)\n",
    "#             print(\"self.output:\",self.output)\n",
    "        return self.output, self.weightsx,self.weightsx1\n",
    "    def _get(self):\n",
    "        z2, w1,w2 = self._gconv(self)\n",
    "#         with tf.Session() as sess:  print(w.eval())\n",
    "        return z2, w1,w2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_Model(train_ST_emb_reshaped,train_Ex_emb,Adj_distance1,Adj_Speed1,Adj_Flow1,weights_G1,weights1_G1):\n",
    "\n",
    "    numerics  = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64', 'uint8']\n",
    "    input_ST0 = tf.convert_to_tensor(train_ST_emb_reshaped, dtype=tf.dtypes.float32)\n",
    "    input_XT0 = tf.convert_to_tensor(train_Ex_emb,dtype=tf.dtypes.float32)\n",
    "    \n",
    "    Adj_distance2 = Adj_distance1\n",
    "    Adj_Speed2 = Adj_Speed1\n",
    "    Adj_Flow2  = Adj_Flow1\n",
    "#     print(f'Adj_distance2 is {Adj_distance2[0][0:5]}')\n",
    "    \n",
    "    weights_G2 = weights_G1\n",
    "    weights2_G2= weights1_G1\n",
    "\n",
    "    \n",
    "    ###### Spatial-Temporal Features module:    \n",
    "    ### TCN Block1 for Spatial-Temporal\n",
    "#     input_shape1 = tf.keras.Input(shape=(train_ST_emb_reshaped.shape[1],train_ST_emb_reshaped.shape[2]))\n",
    "    TCN1 = tf.keras.layers.Conv1D(filters=2, kernel_size=5, strides=1, dilation_rate=1, padding='causal', \n",
    "                                    name='Conv1D_TCN1')(input_ST0)\n",
    "    TCN1 = tf.keras.activations.relu(TCN1)\n",
    "    TCN1 = tf.keras.layers.Dropout(0.0)(TCN1)\n",
    "    TCN1 = tf.keras.layers.MaxPooling1D(pool_size=4,strides=1)(TCN1)\n",
    "#     print(\"The TCN1 output is \", TCN1.shape)\n",
    "    \n",
    "    ### TCN Block2 for Spatial-Local\n",
    "    TCN2 = tf.keras.layers.Conv1D(filters=2, kernel_size=5, strides=1, dilation_rate=2, padding='causal', \n",
    "                                     name='Conv1D_TCN2')(input_ST0)\n",
    "    TCN2 = tf.keras.activations.relu(TCN2)\n",
    "    TCN2 = tf.keras.layers.Dropout(0.0)(TCN2)\n",
    "    TCN2 = tf.keras.layers.AveragePooling1D(pool_size=4,strides=1)(TCN2)\n",
    "#     print(\"The TCN2 output is \", TCN2.shape)\n",
    "    \n",
    "    ### TCN Block3 for combination features\n",
    "#     combination = Concatenate(axis=1)([input_shape1, input_shape2])\n",
    "    TCN3 = tf.keras.layers.Conv1D(filters=2, kernel_size=5, strides=1, dilation_rate=4, padding='causal', \n",
    "                                     name='Conv1D_TCN3')(input_ST0)\n",
    "    TCN3 = tf.keras.activations.relu(TCN3)\n",
    "    TCN3 = tf.keras.layers.Dropout(0.0)(TCN3)\n",
    "    TCN3 = tf.keras.layers.MaxPooling1D(pool_size=4,strides=1)(TCN3)\n",
    "#     print(\"The TCN3 output is \", TCN3.shape)\n",
    "    \n",
    "    \n",
    "    #### Self-attention module for Spatio-temporal correlations\n",
    "    TCN_Mul1  = tf.keras.layers.Multiply()([ TCN1, TCN2])\n",
    "    TCN_Mul2  = tf.keras.layers.Multiply()([ TCN_Mul1, TCN3])\n",
    "#     print(\"The Spatial-Temporal Features module output is \", TCN_Mul2.shape)\n",
    "    TCN_Mul2 = tf.keras.layers.Flatten()(TCN_Mul2)\n",
    "#     print(\"The Spatial-Temporal Features module output is \", TCN_Mul2.shape)\n",
    "    \n",
    "    \n",
    "    ##### Spatial-Temporal Graphs module:    \n",
    "    ### Speed Graph Block\n",
    "    GGCN_STS = G_GCN(135, Adj_Speed2, TCN_Mul2, 135, 66,weights_G2,weights2_G2)\n",
    "    GGCN_STS,w1,w2 = GGCN_STS._get()\n",
    "#     print(\"GGCN_STS:\", GGCN_STS)\n",
    "    \n",
    "    \n",
    "    ### Distance Graph Block\n",
    "    GGCN_STD = G_GCN(135, Adj_distance2, TCN_Mul2, 135, 66,weights_G2,weights2_G2)\n",
    "    GGCN_STD,w1,w2 = GGCN_STD._get()\n",
    "#     print(\"GGCN_STD:\", GGCN_STD)\n",
    "    \n",
    "    \n",
    "    ### Flow Graph Block\n",
    "    GGCN_STF = G_GCN(135, Adj_Flow2, TCN_Mul2, 135, 66,weights_G2,weights2_G2)\n",
    "    GGCN_STF,w1,w2 = GGCN_STF._get()\n",
    "#     print(\"GGCN_STF:\", GGCN_STF)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #### Weight sharing layers for calculate similarity\n",
    "    Weight_layer1 = Lambda(function=lambda x: 1 - K.abs(x[0] - x[1]), output_shape=lambda x: x[0],\n",
    "                    name='L1_distance1')([GGCN_STS, GGCN_STD])\n",
    "    Weight_layer2 = Lambda(function=lambda x: 1 - K.abs(x[0] - x[1]), output_shape=lambda x: x[0],\n",
    "                    name='L1_distance2')([GGCN_STD, GGCN_STF])\n",
    "    Weight_layer2 = tf.transpose(Weight_layer2, perm=[0,2,1])\n",
    "    \n",
    "#     print(\"Weight_layer1:\", Weight_layer1)\n",
    "#     print(\"Weight_layer2:\", Weight_layer2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "     ####TCN_Cross Gated module for Graph Fusion representation:\n",
    "    Cross_Mul  = tf.matmul(Weight_layer1,Weight_layer2)\n",
    "#     Cross_Mul  = tf.keras.layers.Multiply()([ Weight_layer1, Weight_layer2])\n",
    "# #     Cross_Mul  = tf.transpose(Cross_Mul, perm=[0,2,1])\n",
    "# #     Cross_Mul = tf.reshape(Cross_Mul, shape=[Cross_Mul.shape[0], Cross_Mul.shape[1],1])\n",
    "#     print(\"Cross_Mul:\", Cross_Mul)\n",
    "#     Cross_Mul2 = tf.keras.layers.Flatten()(Cross_Mul)\n",
    "#     print(\"Cross_Mul2:\", Cross_Mul2)\n",
    "\n",
    "    TCN4 = tf.keras.layers.Conv1D(filters=32, kernel_size=5, strides=1, dilation_rate=3, padding='causal', \n",
    "                                    name='Conv1D_TCN4')(Cross_Mul)\n",
    "    TCN4 = tf.keras.activations.relu(TCN4)\n",
    "    TCN4 = tf.keras.layers.Dropout(0.0)(TCN4)\n",
    "    TCN4 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1)(TCN4)\n",
    "#     print(\"The TCN4 output is \", TCN4.shape)\n",
    "    \n",
    "    TCN5 = tf.keras.layers.Conv1D(filters=32, kernel_size=5, strides=1, dilation_rate=2, padding='causal', \n",
    "                                    name='Conv1D_TCN5')(Cross_Mul)\n",
    "    TCN5 = tf.keras.activations.relu(TCN5)\n",
    "    TCN5 = tf.keras.layers.Dropout(0.0)(TCN5)\n",
    "    TCN5 = tf.keras.layers.AveragePooling1D(pool_size=2,strides=1)(TCN5)\n",
    "#     print(\"The TCN5 output is \", TCN5.shape)\n",
    "    \n",
    "#     Graph_out = Lambda(function=lambda x: 1 - K.abs(x[0] - x[1]), output_shape=lambda x: x[0],\n",
    "#                     name='L1_distance1')([TCN4, TCN5])\n",
    "    \n",
    "    \n",
    "    Graph_out  = Concatenate(axis=1)([TCN4, TCN5])\n",
    "#     Graph_out  = tf.keras.layers.Multiply()([ TCN4, TCN5])\n",
    "#     print(\"The Spatial-Temporal Graph module output is \", Graph_out.shape)\n",
    "    Graph_out = tf.keras.layers.Flatten()(Graph_out)\n",
    "#     print(\"The Spatial-Temporal Graph module output after flatten is \", Graph_out.shape)\n",
    "    \n",
    "\n",
    "    ####DualGated ResNet module for External features:\n",
    "#     input_shape2 = tf.keras.Input(shape=(train_Ex_emb.shape[1],))\n",
    "    Ext_layer1 = Dense(64,use_bias=False,kernel_initializer=\"he_uniform\", activation= 'relu',\n",
    "               kernel_regularizer= regularizers.l2(0.001))(input_XT0)\n",
    "    Ext_layer2 = Dense(64,use_bias=False,kernel_initializer=\"he_uniform\", activation= 'relu',\n",
    "               kernel_regularizer= regularizers.l2(0.001))(input_XT0)\n",
    "    Block1_out1= tf.keras.layers.Multiply()([Ext_layer1, Ext_layer2])\n",
    "#     BN_Layer   = LayerNormalization(epsilon=1e-06, weights=None, trainable=False)(Block1_out1,training=True)\n",
    "    Ext_layer3 = Dense(128,use_bias=False,kernel_initializer=\"he_uniform\", activation= 'relu',\n",
    "               kernel_regularizer= regularizers.l2(0.001))(Block1_out1)\n",
    "    Ext_layer4 = Dense(128,use_bias=False,kernel_initializer=\"he_uniform\", activation= 'relu',\n",
    "               kernel_regularizer= regularizers.l2(0.001))(Block1_out1)\n",
    "    Block1_out2= Concatenate(axis=1)([Ext_layer3, Ext_layer4])\n",
    "    Block1_out2 = tf.keras.layers.Flatten()(Block1_out2)\n",
    "\n",
    "\n",
    "    ######Gated Fusion Module for ALL Features representation \n",
    "    Stage1 = Dense(256,use_bias=False,kernel_initializer=\"he_uniform\", activation= 'relu',\n",
    "               kernel_regularizer= regularizers.l2(0.001))(Graph_out)\n",
    "#     Stage1_flatten = tf.keras.layers.Flatten()(Stage1)\n",
    "#     Stage2_flatten = tf.keras.layers.Flatten()(Block1_out2)\n",
    "    Stage2 = Dense(128,use_bias=False,kernel_initializer=\"he_uniform\", activation= 'relu',\n",
    "               kernel_regularizer= regularizers.l2(0.001))(Block1_out2)\n",
    "    Geted_concat = Concatenate(axis=1)([Stage1, Stage2])\n",
    "#     print(f\"Geted_concat output shape is {Geted_concat.shape}\")\n",
    "    \n",
    "    \n",
    "#     #### Transformer MHA Module\n",
    "#     self_hidden_size = Geted_concat.shape[1]\n",
    "#     Final_MHA_Block = InterpretableMultiHeadAttention(num_heads, self_hidden_size, dropout=0.0)\n",
    "#     Final_MHA_Block_output, Final_MHA_self_att  = Final_MHA_Block(Geted_concat, Geted_concat, Geted_concat, mask=None)\n",
    "#     print(f\"Final_MHA_Block_output shape is {Final_MHA_Block_output.shape}\")\n",
    "    \n",
    "    \n",
    "\n",
    "    #####FC layers and Dense for output######\n",
    "    # Layer 1\n",
    "#     FC = LayerNormalization(epsilon=1e-06, weights=None, trainable=False)(Geted_concat,training=True)\n",
    "    FC = Dense(128,use_bias=False,kernel_initializer=\"he_uniform\", name = \"FC1\",\n",
    "               kernel_regularizer= regularizers.l2(0.01))(Geted_concat)\n",
    "#     FC = LayerNormalization(epsilon=1e-06, weights=None, trainable=False)(FC,training=True)\n",
    "    FC = Activation('relu')(FC)\n",
    "#     FC = LayerNormalization(epsilon=1e-06, weights=None, trainable=False)(FC,training=True)\n",
    "    FC = Dropout(0.1)(FC)\n",
    "    \n",
    "    # Layer 2\n",
    "#     FC = LayerNormalization(epsilon=1e-06, weights=None, trainable=False)(FC,training=True)\n",
    "    FC = Dense(256,use_bias=False,kernel_initializer=\"he_uniform\", name = \"FC2\",\n",
    "               kernel_regularizer= regularizers.l2(0.01))(FC)\n",
    "#     FC = LayerNormalization(epsilon=1e-06, weights=None, trainable=False)(FC,training=True)\n",
    "    FC = Activation('relu')(FC)\n",
    "#     FC = LayerNormalization(epsilon=1e-06, weights=None, trainable=False)(FC,training=True)\n",
    "    FC = Dropout(0.1)(FC)\n",
    "    \n",
    "\n",
    "    # Layer 3\n",
    "#     FC = LayerNormalization(epsilon=1e-06, weights=None, trainable=False)(FC,training=True)\n",
    "    FC = Dense(128,use_bias=False,kernel_initializer=\"he_uniform\",name = \"FC3\",\n",
    "               kernel_regularizer= regularizers.l2(0.01))(FC)\n",
    "#     FC = LayerNormalization(epsilon=1e-06, weights=None, trainable=False)(FC,training=True)\n",
    "    FC = Activation('relu')(FC)\n",
    "#     FC = LayerNormalization(epsilon=1e-06, weights=None, trainable=False)(FC,training=True)\n",
    "    FC = Dropout(0.1)(FC)\n",
    "\n",
    "    # Layer 4\n",
    "#     model = LayerNormalization(epsilon=1e-06, weights=None, trainable=False)(FC,training=True)\n",
    "    FC = Dense(64,use_bias=False,kernel_initializer=\"he_uniform\", name = \"FC4\",\n",
    "               kernel_regularizer= regularizers.l2(0.01))(FC)\n",
    "#     FC = LayerNormalization(epsilon=1e-06, weights=None, trainable=False)(FC,training=True)\n",
    "    FC = Activation('relu')(FC)\n",
    "#     FC = LayerNormalization(epsilon=1e-06, weights=None, trainable=False)(FC,training=True)\n",
    "    FC = Dropout(0.1)(FC)\n",
    "    \n",
    "        # Layer 5\n",
    "#     FC = LayerNormalization(epsilon=1e-06, weights=None, trainable=False)(FC,training=True)\n",
    "    FC = Dense(32,use_bias=False,kernel_initializer=\"he_uniform\", name = \"FC5\", \n",
    "               kernel_regularizer= regularizers.l2(0.01))(FC)\n",
    "#     FC = Dropout(0.1)(FC)\n",
    "    # model = BatchNormalization(epsilon=1e-06, momentum=0.99, weights=None, trainable=False)(model,training=True)\n",
    "    FC = Activation('relu')(FC)\n",
    "\n",
    "    \n",
    "    \n",
    "    #Linear Dense Output layer\n",
    "#     mlp_out = LayerNormalization(epsilon=1e-06, weights=None, trainable=False)(FC,training=True)\n",
    "    output = Dense(1,activation='linear')(FC)\n",
    "    \n",
    "    \n",
    "#     output = tf.reshape(output, shape=[-1])\n",
    "#     _weights = tf.reshape(_weights, shape=[-1])\n",
    "#     output = output*_weights + _biases\n",
    "#     output = tf.reshape(output, shape=[-1,1])\n",
    "#     output = tf.cast(output, dtype='float32')\n",
    "    \n",
    "    \n",
    "    return output, w1, w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test model for each hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We start train model\n",
      "We train model for hour 5\n",
      "we start train epoch:0\n",
      "we start train epoch:1\n",
      "we start train epoch:2\n",
      "we start train epoch:3\n",
      "we start train epoch:4\n",
      "we start train epoch:5\n",
      "we start train epoch:6\n",
      "we start train epoch:7\n",
      "we start train epoch:8\n",
      "we start train epoch:9\n",
      "we start train epoch:10\n",
      "we start train epoch:11\n",
      "we start train epoch:12\n",
      "we start train epoch:13\n",
      "we start train epoch:14\n",
      "we start train epoch:15\n",
      "we start train epoch:16\n",
      "we start train epoch:17\n",
      "we start train epoch:18\n",
      "we start train epoch:19\n",
      "we start train epoch:20\n",
      "we start train epoch:21\n",
      "we start train epoch:22\n",
      "we start train epoch:23\n",
      "we start train epoch:24\n",
      "we start train epoch:25\n",
      "we start train epoch:26\n",
      "we start train epoch:27\n",
      "we start train epoch:28\n",
      "we start train epoch:29\n",
      "Hour 5 RMSE:404.97385999104904\n",
      "Hour 5 MAE:276.85099973176654\n",
      "Hour 5 MAPE:39.20193244594637\n",
      "We train model for hour 6\n",
      "we start train epoch:0\n",
      "we start train epoch:1\n",
      "we start train epoch:2\n",
      "we start train epoch:3\n",
      "we start train epoch:4\n",
      "we start train epoch:5\n",
      "we start train epoch:6\n",
      "we start train epoch:7\n",
      "we start train epoch:8\n",
      "we start train epoch:9\n",
      "we start train epoch:10\n",
      "we start train epoch:11\n",
      "we start train epoch:12\n",
      "we start train epoch:13\n",
      "we start train epoch:14\n",
      "we start train epoch:15\n",
      "we start train epoch:16\n",
      "we start train epoch:17\n",
      "we start train epoch:18\n",
      "we start train epoch:19\n",
      "we start train epoch:20\n",
      "we start train epoch:21\n",
      "we start train epoch:22\n",
      "we start train epoch:23\n",
      "we start train epoch:24\n",
      "we start train epoch:25\n",
      "we start train epoch:26\n",
      "we start train epoch:27\n",
      "we start train epoch:28\n",
      "we start train epoch:29\n",
      "Hour 6 RMSE:339.5500965040018\n",
      "Hour 6 MAE:229.3643301296439\n",
      "Hour 6 MAPE:30.573999404598208\n",
      "We train model for hour 7\n",
      "we start train epoch:0\n",
      "we start train epoch:1\n",
      "we start train epoch:2\n",
      "we start train epoch:3\n",
      "we start train epoch:4\n",
      "we start train epoch:5\n",
      "we start train epoch:6\n",
      "we start train epoch:7\n",
      "we start train epoch:8\n",
      "we start train epoch:9\n",
      "we start train epoch:10\n",
      "we start train epoch:11\n",
      "we start train epoch:12\n",
      "we start train epoch:13\n",
      "we start train epoch:14\n",
      "we start train epoch:15\n",
      "we start train epoch:16\n",
      "we start train epoch:17\n",
      "we start train epoch:18\n",
      "we start train epoch:19\n",
      "we start train epoch:20\n",
      "we start train epoch:21\n",
      "we start train epoch:22\n",
      "we start train epoch:23\n",
      "we start train epoch:24\n",
      "we start train epoch:25\n",
      "we start train epoch:26\n",
      "we start train epoch:27\n",
      "we start train epoch:28\n",
      "we start train epoch:29\n",
      "Hour 7 RMSE:277.4885309338941\n",
      "Hour 7 MAE:183.1608939303469\n",
      "Hour 7 MAPE:23.420910424784893\n",
      "We train model for hour 8\n",
      "we start train epoch:0\n",
      "we start train epoch:1\n",
      "we start train epoch:2\n",
      "we start train epoch:3\n",
      "we start train epoch:4\n",
      "we start train epoch:5\n",
      "we start train epoch:6\n",
      "we start train epoch:7\n",
      "we start train epoch:8\n",
      "we start train epoch:9\n",
      "we start train epoch:10\n",
      "we start train epoch:11\n",
      "we start train epoch:12\n",
      "we start train epoch:13\n",
      "we start train epoch:14\n",
      "we start train epoch:15\n",
      "we start train epoch:16\n",
      "we start train epoch:17\n",
      "we start train epoch:18\n",
      "we start train epoch:19\n",
      "we start train epoch:20\n",
      "we start train epoch:21\n",
      "we start train epoch:22\n",
      "we start train epoch:23\n",
      "we start train epoch:24\n",
      "we start train epoch:25\n",
      "we start train epoch:26\n",
      "we start train epoch:27\n",
      "we start train epoch:28\n",
      "we start train epoch:29\n",
      "Hour 8 RMSE:239.9620137621335\n",
      "Hour 8 MAE:155.2445684541886\n",
      "Hour 8 MAPE:19.361658631770922\n",
      "We train model for hour 9\n",
      "we start train epoch:0\n",
      "we start train epoch:1\n",
      "we start train epoch:2\n",
      "we start train epoch:3\n",
      "we start train epoch:4\n",
      "we start train epoch:5\n",
      "we start train epoch:6\n",
      "we start train epoch:7\n",
      "we start train epoch:8\n",
      "we start train epoch:9\n",
      "we start train epoch:10\n",
      "we start train epoch:11\n",
      "we start train epoch:12\n",
      "we start train epoch:13\n",
      "we start train epoch:14\n",
      "we start train epoch:15\n",
      "we start train epoch:16\n",
      "we start train epoch:17\n",
      "we start train epoch:18\n",
      "we start train epoch:19\n",
      "we start train epoch:20\n",
      "we start train epoch:21\n",
      "we start train epoch:22\n",
      "we start train epoch:23\n",
      "we start train epoch:24\n",
      "we start train epoch:25\n",
      "we start train epoch:26\n",
      "we start train epoch:27\n",
      "we start train epoch:28\n",
      "we start train epoch:29\n",
      "Hour 9 RMSE:210.50014030331096\n",
      "Hour 9 MAE:135.3029789697781\n",
      "Hour 9 MAPE:16.673174610008193\n",
      "We train model for hour 10\n",
      "we start train epoch:0\n",
      "we start train epoch:1\n",
      "we start train epoch:2\n",
      "we start train epoch:3\n",
      "we start train epoch:4\n",
      "we start train epoch:5\n",
      "we start train epoch:6\n",
      "we start train epoch:7\n",
      "we start train epoch:8\n",
      "we start train epoch:9\n",
      "we start train epoch:10\n",
      "we start train epoch:11\n",
      "we start train epoch:12\n",
      "we start train epoch:13\n",
      "we start train epoch:14\n",
      "we start train epoch:15\n",
      "we start train epoch:16\n",
      "we start train epoch:17\n",
      "we start train epoch:18\n",
      "we start train epoch:19\n",
      "we start train epoch:20\n",
      "we start train epoch:21\n",
      "we start train epoch:22\n",
      "we start train epoch:23\n",
      "we start train epoch:24\n",
      "we start train epoch:25\n",
      "we start train epoch:26\n",
      "we start train epoch:27\n",
      "we start train epoch:28\n",
      "we start train epoch:29\n",
      "Hour 10 RMSE:191.32956661340847\n",
      "Hour 10 MAE:121.64150676774402\n",
      "Hour 10 MAPE:14.791021407226117\n",
      "We train model for hour 11\n",
      "we start train epoch:0\n",
      "we start train epoch:1\n",
      "we start train epoch:2\n",
      "we start train epoch:3\n",
      "we start train epoch:4\n",
      "we start train epoch:5\n",
      "we start train epoch:6\n",
      "we start train epoch:7\n",
      "we start train epoch:8\n",
      "we start train epoch:9\n",
      "we start train epoch:10\n",
      "we start train epoch:11\n",
      "we start train epoch:12\n",
      "we start train epoch:13\n",
      "we start train epoch:14\n",
      "we start train epoch:15\n",
      "we start train epoch:16\n",
      "we start train epoch:17\n",
      "we start train epoch:18\n",
      "we start train epoch:19\n",
      "we start train epoch:20\n",
      "we start train epoch:21\n",
      "we start train epoch:22\n",
      "we start train epoch:23\n",
      "we start train epoch:24\n",
      "we start train epoch:25\n",
      "we start train epoch:26\n",
      "we start train epoch:27\n",
      "we start train epoch:28\n",
      "we start train epoch:29\n",
      "Hour 11 RMSE:177.88501029475904\n",
      "Hour 11 MAE:112.1514811749656\n",
      "Hour 11 MAPE:13.448060393068248\n",
      "We train model for hour 12\n",
      "we start train epoch:0\n",
      "we start train epoch:1\n",
      "we start train epoch:2\n",
      "we start train epoch:3\n",
      "we start train epoch:4\n",
      "we start train epoch:5\n",
      "we start train epoch:6\n",
      "we start train epoch:7\n",
      "we start train epoch:8\n",
      "we start train epoch:9\n",
      "we start train epoch:10\n",
      "we start train epoch:11\n",
      "we start train epoch:12\n",
      "we start train epoch:13\n",
      "we start train epoch:14\n",
      "we start train epoch:15\n",
      "we start train epoch:16\n",
      "we start train epoch:17\n",
      "we start train epoch:18\n",
      "we start train epoch:19\n",
      "we start train epoch:20\n",
      "we start train epoch:21\n",
      "we start train epoch:22\n",
      "we start train epoch:23\n",
      "we start train epoch:24\n",
      "we start train epoch:25\n",
      "we start train epoch:26\n",
      "we start train epoch:27\n",
      "we start train epoch:28\n",
      "we start train epoch:29\n",
      "Hour 12 RMSE:167.38810142876224\n",
      "Hour 12 MAE:104.33324283338973\n",
      "Hour 12 MAPE:12.356820016410978\n",
      "We train model for hour 13\n",
      "we start train epoch:0\n",
      "we start train epoch:1\n",
      "we start train epoch:2\n",
      "we start train epoch:3\n",
      "we start train epoch:4\n",
      "we start train epoch:5\n",
      "we start train epoch:6\n",
      "we start train epoch:7\n",
      "we start train epoch:8\n",
      "we start train epoch:9\n",
      "we start train epoch:10\n",
      "we start train epoch:11\n",
      "we start train epoch:12\n",
      "we start train epoch:13\n",
      "we start train epoch:14\n",
      "we start train epoch:15\n",
      "we start train epoch:16\n",
      "we start train epoch:17\n",
      "we start train epoch:18\n",
      "we start train epoch:19\n",
      "we start train epoch:20\n",
      "we start train epoch:21\n",
      "we start train epoch:22\n",
      "we start train epoch:23\n",
      "we start train epoch:24\n",
      "we start train epoch:25\n",
      "we start train epoch:26\n",
      "we start train epoch:27\n",
      "we start train epoch:28\n",
      "we start train epoch:29\n",
      "Hour 13 RMSE:158.68184403618864\n",
      "Hour 13 MAE:97.8488597365509\n",
      "Hour 13 MAPE:11.44999814555075\n",
      "We train model for hour 14\n",
      "we start train epoch:0\n",
      "we start train epoch:1\n",
      "we start train epoch:2\n",
      "we start train epoch:3\n",
      "we start train epoch:4\n",
      "we start train epoch:5\n",
      "we start train epoch:6\n",
      "we start train epoch:7\n",
      "we start train epoch:8\n",
      "we start train epoch:9\n",
      "we start train epoch:10\n",
      "we start train epoch:11\n",
      "we start train epoch:12\n",
      "we start train epoch:13\n",
      "we start train epoch:14\n",
      "we start train epoch:15\n",
      "we start train epoch:16\n",
      "we start train epoch:17\n",
      "we start train epoch:18\n",
      "we start train epoch:19\n",
      "we start train epoch:20\n",
      "we start train epoch:21\n",
      "we start train epoch:22\n",
      "we start train epoch:23\n",
      "we start train epoch:24\n",
      "we start train epoch:25\n",
      "we start train epoch:26\n",
      "we start train epoch:27\n",
      "we start train epoch:28\n",
      "we start train epoch:29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hour 14 RMSE:152.7571529775848\n",
      "Hour 14 MAE:92.78227942409282\n",
      "Hour 14 MAPE:10.702180869328597\n",
      "We train model for hour 15\n",
      "we start train epoch:0\n",
      "we start train epoch:1\n",
      "we start train epoch:2\n",
      "we start train epoch:3\n",
      "we start train epoch:4\n",
      "we start train epoch:5\n",
      "we start train epoch:6\n",
      "we start train epoch:7\n",
      "we start train epoch:8\n",
      "we start train epoch:9\n",
      "we start train epoch:10\n",
      "we start train epoch:11\n",
      "we start train epoch:12\n",
      "we start train epoch:13\n",
      "we start train epoch:14\n",
      "we start train epoch:15\n",
      "we start train epoch:16\n",
      "we start train epoch:17\n",
      "we start train epoch:18\n",
      "we start train epoch:19\n",
      "we start train epoch:20\n",
      "we start train epoch:21\n",
      "we start train epoch:22\n",
      "we start train epoch:23\n",
      "we start train epoch:24\n",
      "we start train epoch:25\n",
      "we start train epoch:26\n",
      "we start train epoch:27\n",
      "we start train epoch:28\n",
      "we start train epoch:29\n",
      "Hour 15 RMSE:147.2545696468457\n",
      "Hour 15 MAE:88.35872011268533\n",
      "Hour 15 MAPE:10.062197680589923\n",
      "We train model for hour 16\n",
      "we start train epoch:0\n",
      "we start train epoch:1\n",
      "we start train epoch:2\n",
      "we start train epoch:3\n",
      "we start train epoch:4\n",
      "we start train epoch:5\n",
      "we start train epoch:6\n",
      "we start train epoch:7\n",
      "we start train epoch:8\n",
      "we start train epoch:9\n",
      "we start train epoch:10\n",
      "we start train epoch:11\n",
      "we start train epoch:12\n",
      "we start train epoch:13\n",
      "we start train epoch:14\n",
      "we start train epoch:15\n",
      "we start train epoch:16\n",
      "we start train epoch:17\n",
      "we start train epoch:18\n",
      "we start train epoch:19\n",
      "we start train epoch:20\n",
      "we start train epoch:21\n",
      "we start train epoch:22\n",
      "we start train epoch:23\n",
      "we start train epoch:24\n",
      "we start train epoch:25\n",
      "we start train epoch:26\n",
      "we start train epoch:27\n",
      "we start train epoch:28\n",
      "we start train epoch:29\n",
      "Hour 16 RMSE:142.66632721220836\n",
      "Hour 16 MAE:84.68287146868434\n",
      "Hour 16 MAPE:9.535276772263655\n",
      "We train model for hour 17\n",
      "we start train epoch:0\n",
      "we start train epoch:1\n",
      "we start train epoch:2\n",
      "we start train epoch:3\n",
      "we start train epoch:4\n",
      "we start train epoch:5\n",
      "we start train epoch:6\n",
      "we start train epoch:7\n",
      "we start train epoch:8\n",
      "we start train epoch:9\n",
      "we start train epoch:10\n",
      "we start train epoch:11\n",
      "we start train epoch:12\n",
      "we start train epoch:13\n",
      "we start train epoch:14\n",
      "we start train epoch:15\n",
      "we start train epoch:16\n",
      "we start train epoch:17\n",
      "we start train epoch:18\n",
      "we start train epoch:19\n",
      "we start train epoch:20\n",
      "we start train epoch:21\n",
      "we start train epoch:22\n",
      "we start train epoch:23\n",
      "we start train epoch:24\n",
      "we start train epoch:25\n",
      "we start train epoch:26\n",
      "we start train epoch:27\n",
      "we start train epoch:28\n",
      "we start train epoch:29\n",
      "Hour 17 RMSE:141.35109187039237\n",
      "Hour 17 MAE:82.54172887301418\n",
      "Hour 17 MAPE:9.105294388412453\n",
      "We train model for hour 18\n",
      "we start train epoch:0\n",
      "we start train epoch:1\n",
      "we start train epoch:2\n",
      "we start train epoch:3\n",
      "we start train epoch:4\n",
      "we start train epoch:5\n",
      "we start train epoch:6\n",
      "we start train epoch:7\n",
      "we start train epoch:8\n",
      "we start train epoch:9\n",
      "we start train epoch:10\n",
      "we start train epoch:11\n",
      "we start train epoch:12\n",
      "we start train epoch:13\n",
      "we start train epoch:14\n",
      "we start train epoch:15\n",
      "we start train epoch:16\n",
      "we start train epoch:17\n",
      "we start train epoch:18\n",
      "we start train epoch:19\n",
      "we start train epoch:20\n",
      "we start train epoch:21\n",
      "we start train epoch:22\n",
      "we start train epoch:23\n",
      "we start train epoch:24\n",
      "we start train epoch:25\n",
      "we start train epoch:26\n",
      "we start train epoch:27\n",
      "we start train epoch:28\n",
      "we start train epoch:29\n",
      "Hour 18 RMSE:140.63637047259985\n",
      "Hour 18 MAE:80.96133462028158\n",
      "Hour 18 MAPE:8.75426021932226\n",
      "We train model for hour 19\n",
      "we start train epoch:0\n",
      "we start train epoch:1\n",
      "we start train epoch:2\n",
      "we start train epoch:3\n",
      "we start train epoch:4\n",
      "we start train epoch:5\n",
      "we start train epoch:6\n",
      "we start train epoch:7\n",
      "we start train epoch:8\n",
      "we start train epoch:9\n",
      "we start train epoch:10\n",
      "we start train epoch:11\n",
      "we start train epoch:12\n",
      "we start train epoch:13\n",
      "we start train epoch:14\n",
      "we start train epoch:15\n",
      "we start train epoch:16\n",
      "we start train epoch:17\n",
      "we start train epoch:18\n",
      "we start train epoch:19\n",
      "we start train epoch:20\n",
      "we start train epoch:21\n",
      "we start train epoch:22\n",
      "we start train epoch:23\n",
      "we start train epoch:24\n",
      "we start train epoch:25\n",
      "we start train epoch:26\n",
      "we start train epoch:27\n",
      "we start train epoch:28\n",
      "we start train epoch:29\n",
      "Hour 19 RMSE:138.08269754881314\n",
      "Hour 19 MAE:78.72391416403808\n",
      "Hour 19 MAPE:8.431980542723256\n",
      "We train model for hour 20\n",
      "we start train epoch:0\n",
      "we start train epoch:1\n",
      "we start train epoch:2\n",
      "we start train epoch:3\n",
      "we start train epoch:4\n",
      "we start train epoch:5\n",
      "we start train epoch:6\n",
      "we start train epoch:7\n",
      "we start train epoch:8\n",
      "we start train epoch:9\n",
      "we start train epoch:10\n",
      "we start train epoch:11\n",
      "we start train epoch:12\n",
      "we start train epoch:13\n",
      "we start train epoch:14\n",
      "we start train epoch:15\n",
      "we start train epoch:16\n",
      "we start train epoch:17\n",
      "we start train epoch:18\n",
      "we start train epoch:19\n",
      "we start train epoch:20\n",
      "we start train epoch:21\n",
      "we start train epoch:22\n",
      "we start train epoch:23\n",
      "we start train epoch:24\n",
      "we start train epoch:25\n",
      "we start train epoch:26\n",
      "we start train epoch:27\n",
      "we start train epoch:28\n",
      "we start train epoch:29\n",
      "Hour 20 RMSE:135.56941416242248\n",
      "Hour 20 MAE:76.69950980906725\n",
      "Hour 20 MAPE:8.162858716735897\n",
      "We train model for hour 21\n",
      "we start train epoch:0\n",
      "we start train epoch:1\n",
      "we start train epoch:2\n",
      "we start train epoch:3\n",
      "we start train epoch:4\n",
      "we start train epoch:5\n",
      "we start train epoch:6\n",
      "we start train epoch:7\n",
      "we start train epoch:8\n",
      "we start train epoch:9\n",
      "we start train epoch:10\n",
      "we start train epoch:11\n",
      "we start train epoch:12\n",
      "we start train epoch:13\n",
      "we start train epoch:14\n",
      "we start train epoch:15\n",
      "we start train epoch:16\n",
      "we start train epoch:17\n",
      "we start train epoch:18\n",
      "we start train epoch:19\n",
      "we start train epoch:20\n",
      "we start train epoch:21\n",
      "we start train epoch:22\n",
      "we start train epoch:23\n",
      "we start train epoch:24\n",
      "we start train epoch:25\n",
      "we start train epoch:26\n",
      "we start train epoch:27\n",
      "we start train epoch:28\n",
      "we start train epoch:29\n",
      "Hour 21 RMSE:134.89829404345085\n",
      "Hour 21 MAE:75.47309094932584\n",
      "Hour 21 MAPE:7.96173328404758\n",
      "We train model for hour 22\n",
      "we start train epoch:0\n",
      "we start train epoch:1\n",
      "we start train epoch:2\n",
      "we start train epoch:3\n",
      "we start train epoch:4\n",
      "we start train epoch:5\n",
      "we start train epoch:6\n",
      "we start train epoch:7\n",
      "we start train epoch:8\n",
      "we start train epoch:9\n",
      "we start train epoch:10\n",
      "we start train epoch:11\n",
      "we start train epoch:12\n",
      "we start train epoch:13\n",
      "we start train epoch:14\n",
      "we start train epoch:15\n",
      "we start train epoch:16\n",
      "we start train epoch:17\n",
      "we start train epoch:18\n",
      "we start train epoch:19\n",
      "we start train epoch:20\n",
      "we start train epoch:21\n",
      "we start train epoch:22\n",
      "we start train epoch:23\n",
      "we start train epoch:24\n",
      "we start train epoch:25\n",
      "we start train epoch:26\n",
      "we start train epoch:27\n",
      "we start train epoch:28\n",
      "we start train epoch:29\n",
      "Hour 22 RMSE:133.4496714691305\n",
      "Hour 22 MAE:74.37183364173592\n",
      "Hour 22 MAPE:7.820339351857181\n",
      "Model RMSE =  190.80137518171975\n",
      "Model MAE  =  119.47189693284999\n",
      "Model MAPE =  14.545205405813638\n"
     ]
    }
   ],
   "source": [
    "####Delete all flags before declare#####\n",
    "# batch_size = 16\n",
    "\n",
    "# for name in list(flags.FLAGS):\n",
    "#       delattr(flags.FLAGS,name)\n",
    "\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "###### Settings ######\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_float('learning_rate', 0.001, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('training_epoch', 30, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('batch_size', 32, 'batch size.')\n",
    "batch_size = FLAGS.batch_size\n",
    "lr = FLAGS.learning_rate\n",
    "training_epoch = FLAGS.training_epoch\n",
    "num_nodes = 135\n",
    "# batch_size = 16\n",
    "\n",
    "labels = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "\n",
    "# ADJ = tf.placeholder(shape=[135, 135], dtype=tf.float64)\n",
    "\n",
    "inputs1 = tf.placeholder(shape=[None, 36,1], dtype=tf.float32)\n",
    "inputs2 = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "inputs  = [inputs1, inputs2]\n",
    "\n",
    "\n",
    "weights = weight_variable_glorot(66,135, name='weights')\n",
    "weights1 = weight_variable_glorot(135,135, name='weights1')\n",
    "\n",
    "\n",
    "# weights = {\n",
    "#     'out': tf.Variable(tf.random_normal([64, 1], mean=1.0),dtype=tf.float32, name='weight_o')}\n",
    "# biases = {\n",
    "#     'out': tf.Variable(tf.random_normal([1]),dtype=tf.float32,name='bias_o')}\n",
    "\n",
    "##First hour (define i as = 5) \n",
    "i = 5 \n",
    "# hr = tf.convert_to_tensor(10, dtype=tf.int32)\n",
    "# dftrain1 = train_ST_emb_reshaped[0:batch_size]\n",
    "# dftrain2 = train_Ex_emb[0:batch_size]\n",
    "\n",
    "# train_ST_emb_reshaped,train_Ex_emb\n",
    "# pred, w1,b1, w2,b2 = Build_Model(df1, \n",
    "#                Adj_distance[i],Adj_Speed[i],Adj_Flow[i], weights_ST, bias_ST, weights_ST1, bias_ST1,weights ,biases)\n",
    "\n",
    "pred,w1,w2 = Build_Model(inputs1,inputs2,Adj_distance[i],Adj_Speed[i],Adj_Flow[i],weights,weights1)\n",
    "y_pred = pred\n",
    "\n",
    "##loss\n",
    "mae_loss = tf.keras.losses.MeanAbsoluteError()\n",
    "loss =  tf.reduce_mean(mae_loss(y_pred,labels))\n",
    "# loss = tf.reduce_mean(tf.keras.metrics.mean_absolute_error(y_pred,labels))\n",
    "# loss = tf.reduce_mean(tf.square(y_pred-labels))\n",
    "# loss = tf.reduce_mean(tf.squared_difference(y_pred, labels))\n",
    "##rmse\n",
    "\n",
    "# error = tf.reduce_mean(tf.abs(labels - y_pred))\n",
    "error = tf.reduce_mean(tf.keras.metrics.mean_absolute_error(labels, y_pred))\n",
    "# error = tf.reduce_mean(tf.square(y_pred-labels))\n",
    "optimizer = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "\n",
    "\n",
    "###### Initialize session ######\n",
    "# variables = tf.initialize_all_variables()\n",
    "variables = tf.global_variables()\n",
    "saver = tf.train.Saver(tf.global_variables())  \n",
    "#sess = tf.Session()\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "def MAPE(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "\n",
    "# path = './model'\n",
    "    \n",
    "###### evaluation ######\n",
    "def evaluation(a,b):\n",
    "    rmse = math.sqrt(mean_squared_error(a,b))\n",
    "    mae  = mean_absolute_error(a,b)\n",
    "    mape = MAPE (a,b)\n",
    "#     F_norm = la.norm(a-b,'fro')/la.norm(a,'fro')\n",
    "#     r2  = 1-((a-b)**2).sum()/((a-a.mean())**2).sum()\n",
    "#     var = 1-(np.var(a-b))/np.var(a)\n",
    "    return rmse, mae, mape\n",
    "\n",
    "H_RMSE, H_MAE, H_MAPE = [], [], []\n",
    "model_rmse, model_mae, model_mape = [], [], []\n",
    "x_axe,batch_loss,batch_rmse,batch_pred = [], [], [], []\n",
    "test_loss,test_rmse,test_mae,test_pred, test_map = [],[],[],[],[]\n",
    "# time_start = time.time()\n",
    "print(f'We start train model')\n",
    "for i in range(5,23):\n",
    "    train_ST_emb_reshaped, train_Ex_emb, df_trainy   = pre_process(df_Train, i)\n",
    "    test_ST_emb_reshaped, test_Ex_emb, df_testy       = pre_process(df_Test, i)\n",
    "    totalbatch = int(train_ST_emb_reshaped.shape[0]/batch_size)\n",
    "    training_data_count = len(train_ST_emb_reshaped)\n",
    "    totalbatchT = int(test_ST_emb_reshaped.shape[0]/batch_size)\n",
    "    MAE1  = np.empty([totalbatchT], dtype=float)\n",
    "    MAE1.fill(1000)\n",
    "    RMSE1 = np.empty([totalbatchT], dtype=float)\n",
    "    RMSE1.fill(1000)\n",
    "    MAPE1  = np.empty([totalbatchT], dtype=float)\n",
    "    MAPE1.fill(1000)\n",
    "    \n",
    "    print(f'We train model for hour {i}')\n",
    "#     print(f'train_size= ({train_ST_emb_reshaped.shape[0]},{train_Ex_emb.shape[0]}, {df_trainy.shape[0]})')\n",
    "#     print(f'test_size= ({test_ST_emb_reshaped.shape[0]},{test_Ex_emb.shape[0]}, {df_testy.shape[0]})')\n",
    "    if i ==5:\n",
    "        pred,w1,w2 = Build_Model(inputs1,inputs2,Adj_distance[i-5],Adj_Speed[i-5],Adj_Flow[i-5],weights,weights1)\n",
    "    else:\n",
    "        pred,w1,w2 = Build_Model(inputs1,inputs2,Adj_distance[i-5],Adj_Speed[i-5],Adj_Flow[i-5],w1,w2)\n",
    "    for epoch in range(training_epoch):\n",
    "        print(f'we start train epoch:{epoch}')\n",
    "#         st_epoch = time.time()\n",
    "        for m in range(totalbatch):\n",
    "            mini_batch1 = train_ST_emb_reshaped[m * batch_size : (m+1) * batch_size]\n",
    "            mini_label  = df_trainy[m * batch_size : (m+1) * batch_size]\n",
    "            mini_batch2 = train_Ex_emb[m * batch_size : (m+1) * batch_size]\n",
    "            op, loss1, rmse1, train_output = sess.run([optimizer, loss, error, y_pred],\n",
    "                                                     feed_dict = {inputs[0]:mini_batch1, inputs[1]:mini_batch2, labels:mini_label})\n",
    "            batch_loss.append(loss1)\n",
    "#             max_value = np.max(mini_label) \n",
    "            batch_rmse.append(rmse1)\n",
    "#             et_epoch = time.time()\n",
    "#             epoch_time = et_epoch - st_epoch\n",
    "#         print(f'batch RMSE:{rmse1}')\n",
    "#         print(f'Train_batch Loss:{loss1}')\n",
    "#         op.get_weights()\n",
    "#         print(f'batch ACC:{acc}')\n",
    "#     print(f'w is:',WW[0][0][0:5])\n",
    "#         print(f\"Epoch time {epoch} is: {epoch_time} sec\")\n",
    "\n",
    "        test_lossb, test_predb,test_rmseb,test_maeb = [],[],[],[]\n",
    "    \n",
    "        for m in range(totalbatchT):\n",
    "            mini_batchT1 = test_ST_emb_reshaped[m * batch_size : (m+1) * batch_size]\n",
    "            mini_batchT2 = test_Ex_emb[m * batch_size : (m+1) * batch_size]\n",
    "            mini_labelT = df_testy[m * batch_size : (m+1) * batch_size]\n",
    "#         test_T.append(mini_batchT)\n",
    "#         testy_T.append(mini_labelT)\n",
    "#     test_T = np.array(test_T)\n",
    "#     testy_T = np.array(testy_T)\n",
    "            lossb, rmseb, test_outputb = sess.run([loss, error, y_pred],\n",
    "                                         feed_dict = {inputs[0]:mini_batchT1, inputs[1]:mini_batchT2,labels:mini_labelT})\n",
    "            test_labelb = mini_labelT.values\n",
    "#             print(f'Test_batch Loss:{lossb}')\n",
    "            rmseb, maeb,mape  = evaluation(test_labelb, test_outputb)\n",
    "            if maeb < MAE1[m]:\n",
    "                MAE1[m] = maeb\n",
    "            if rmseb < RMSE1[m]:\n",
    "                RMSE1[m] = rmseb\n",
    "            if mape < MAPE1[m]:\n",
    "                MAPE1[m] = mape\n",
    "          \n",
    "      \n",
    "    H_RMSE.append(RMSE1)\n",
    "    H_MAE.append(MAE1)\n",
    "    H_MAPE.append(MAPE1)\n",
    "#         rmse = np.mean(np.array(test_rmseb))  \n",
    "#         mae = np.mean(np.array(test_maeb)) \n",
    "#         acc = np.mean(np.array(test_accb)) \n",
    "#         r2_score = np.mean(np.array(test_r2b)) \n",
    "#         var_score = np.mean(np.array(test_varb)) \n",
    "#     test_predb = np.array(test_predb).ravel()\n",
    "#     test_lossb = np.array(test_lossb).ravel()\n",
    "#     test_label = df_newT[Target].values\n",
    "#     max_value1 = np.max(df_newT[Target]) \n",
    "#     test_label1 = test_label \n",
    "#     test_output1 = test_predb \n",
    "#     test_loss.append(test_lossb)\n",
    "#         test_rmse.append(rmse )\n",
    "#         test_mae.append(mae)\n",
    "#         test_acc.append(acc)\n",
    "#         test_r2.append(r2_score)\n",
    "#         test_var.append(var_score)\n",
    "#     test_pred.append(test_output1)\n",
    "    R,M,P =[],[],[]\n",
    "    for i in range(len(H_RMSE)):\n",
    "        R.append(np.mean(np.array(H_RMSE[i])))\n",
    "        M.append(np.mean(np.array(H_MAE[i])))\n",
    "        P.append(np.mean(np.array(H_MAPE[i])))\n",
    "\n",
    "    model_rmse.append(np.mean(np.array(R)))\n",
    "    model_mae.append(np.mean(np.array(M)))\n",
    "    model_mape.append(np.mean(np.array(P)))\n",
    "    print(f'Hour {i+5} RMSE:{np.mean(np.array(R))}')\n",
    "    print(f'Hour {i+5} MAE:{np.mean(np.array(M))}')\n",
    "    print(f'Hour {i+5} MAPE:{np.mean(np.array(P))}')\n",
    "            \n",
    "#     h_RMSE = np.min(test_rmse)\n",
    "#     h_MAE  = np.min(test_mae)\n",
    "#     print(f'RMSE for Hour is : {h_RMSE}')\n",
    "#     print(f'MAE for Hour is : {h_MAE}')\n",
    "\n",
    "        \n",
    "#     H_RMSE.append(h_RMSE)\n",
    "#     H_MAE.append(h_MAE)\n",
    "#     h_RMSE = 0\n",
    "#     h_MAE  = 0\n",
    "#     h_ACC  = 0\n",
    "        \n",
    "\n",
    "        \n",
    "print(\"Model RMSE = \", np.mean(np.array(model_rmse)))\n",
    "print(\"Model MAE  = \", np.mean(np.array(model_mae)))\n",
    "print(\"Model MAPE = \", np.mean(np.array(model_mape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
